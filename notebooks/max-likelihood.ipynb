{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Principle of Maximum Likelihood\n",
    "- Hayley Song\n",
    "- Aug 29, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Logistic Regression\n",
    "We discuss how we use probability as the language to turn our colloquial questions on a variable in interest to a mathematical notation,\n",
    "and discuss the Bayesian view on estimator, We break down the  discussion into two cases, depending on whether the observations (ie. data) \n",
    "are labeled or not. \n",
    "As a foundation for our discussion, we will discuss what it means to have a model for the probabilistic expression of the question we are asking\n",
    "about the variable of interest, and concrete examples of elementary models for the problems at hand. \n",
    "\n",
    "During the discussion, we will introduce\n",
    "- Probabilistic model: What is a parameterization?\n",
    "- Likelihood of a parameter\n",
    "- Common tasks using this framework\n",
    "    - Prediction\n",
    "         - Unsupervised setting: what is the probability that $X = x$ given our model parametrized by $\\theta$ and observations $D=\\{X^1, ..., X^n\\}$ sometimes, we just want to know the most probable value for $X$\n",
    "        - Supervised setting: what is the probability that $Y=y$ given our model parametrized by $\\theta$ and observations $D=\\{(X^1, Y^1), ... (X^n, Y^n)\\}$\n",
    "\n",
    "- Principle of maximum likelihood to choose the best parameter within the family of models we have chosen. Choosing a good $\\theta$ is also called \"learning\".\n",
    "In the first case, we will introduce what a \"Likelihood\" of a parameter \n",
    "\n",
    "1. Unlabeled data\n",
    "\n",
    "$ D = \\{(X^1, X^2, ..., X^n\\} $\n",
    "\n",
    "This is the case when we observe a variable (eg. Water level of Jeonju-cheon, Height of a student, the side of coin after flipping, etc). Without conditioning it on any other variable, we want to know what is the underlying process that is generating the sample/observation/realization.\n",
    "This is also known as unsupervised setting.\n",
    "     \n",
    "- If $X$ is a binary random variable, the simplest choice of the distribution of $X$ is $\\text{Bern(} \\theta)$: $Pr(X=1 \\vert \\theta) = \\theta$\n",
    "- If we pursue a Bayesian point of view on $\\theta$, we can further discuss the prior and posterior distribution over $\\theta$. \n",
    "- If we choose a conjugate pair of Likelihood model and prior distribution (eg. Bernoulli-Beta model), the posterior distribution over $\\theta$ stays within the same family of distribution as the prior distribution. \n",
    "- As examples, we will discuss Bern-Beta model (for a binary ran.var $X$) and Categorical-Dirichlet model (for $X$ multi-nomial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "2. Labeled data\n",
    "\n",
    "$D = \\{(X^1, Y^1), (X^2, Y^2), ..., (X^n Y^n)\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "This is the case when we are looking some input $X$ and its \"label\" $Y$. \n",
    "\n",
    "- $X$ may be a RGB image and $Y$ may be its category in the set of all possible categories $C = \\{\\text{'ANIMAL', 'PERSON', 'BUILDING', 'NATURE', 'OTHERS'}\\}$.  \n",
    "\n",
    "- $X$ is a document (eg. news article) and $Y$ may be a genre of the document, which must be one of the categories in $C = \\{\\text{'BIZ', 'Entertainment', 'Sports'}\\}$\n",
    "\n",
    "- $Y$ doesn't have to be 1-dimensional.  For semantic segmentation, which is the task of classifying each pixel in the input image $x$ into one of the labels, $Y$ is a 2-dimensional array.\n",
    "\n",
    "We will introduce the notion of **Conditional** likelihood\n",
    "Discuss the difference between the cases of unlabeled  and labeled data, in terms of the independence of random variables\n",
    "In particular, conditional likelihood is more forgiving to the distribution of the input data X's. In fact, we don't need to take this into our model at all, since we will always start with some given input X\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:earthml_v2]",
   "language": "python",
   "name": "conda-env-earthml_v2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
